{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport seaborn as sns\nfrom bs4 import BeautifulSoup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Short description of data**","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ration between classes**","metadata":{}},{"cell_type":"code","source":"sns.countplot(df.sentiment)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check for null values**","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes= {\n    'positive': 1,\n    'negative': 0\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cleaning data -- remove html tags, urls, special symbols, miltiple spaces, spaces at the beginning, single chars and stopwords**","metadata":{}},{"cell_type":"code","source":"stop = set(stopwords.words('english'))\ndef clean_data(text):\n    text = BeautifulSoup(text, \"html.parser\").get_text()\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub(r'\\W', ' ', text)\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text) \n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    \n    final = []\n    for word in text.split():\n        if word.strip().lower() not in stop and word.strip().lower().isalpha():\n            final.append(word.strip().lower())\n    text = \" \".join(final)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.review=df.review.apply(clean_data)\ndf.sentiment = df.sentiment.map(classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Word cloud for positive reviews**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15,15)) # Positive Review Text\nword_cloud = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 1].review))\nplt.imshow(word_cloud , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Word cloud for negative reviews**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15,15)) # Positive Review Text\nword_cloud = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 0].review))\nplt.imshow(word_cloud , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.review.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df.sentiment.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dtm2wid(dtm, maxlen):\n    x = []\n    nwds = []\n    for idx, row in enumerate(dtm):\n        seq = []\n        indices = (row.indices + 1).astype(np.int64)\n        np.append(nwds, len(indices))\n        data = (row.data).astype(np.int64)\n        count_dict = dict(zip(indices, data))\n        for k,v in count_dict.items():\n            seq.extend([k]*v)\n        num_words = len(seq)\n        nwds.append(num_words)\n        # pad up to maxlen with 0\n        if num_words < maxlen: \n            seq = np.pad(seq, (maxlen - num_words, 0),    \n                         mode='constant')\n        # truncate down to maxlen\n        else:                  \n            seq = seq[-maxlen:]\n        x.append(seq)\n    nwds = np.array(nwds)\n#     print('sequence stats: avg:%s, max:%s, min:%s' % (nwds.mean(),\n#                                                       nwds.max(), \n#                                                       nwds.min()) )\n    return np.array(x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Make document-term matrix, word id sequences for train and test**","metadata":{}},{"cell_type":"code","source":"def prepare_data(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)   \n    vectorizer =  CountVectorizer(ngram_range=(1,3), binary=True, \n                             token_pattern=r'\\w+',\n                             max_features=800000)\n    dtm_train = vectorizer.fit_transform(X_train)\n    dtm_test = vectorizer.transform(X_test)\n#     print(\"DTM shape (training): (%s, %s)\" % (dtm_train.shape))\n#     print(\"DTM shape (test): (%s, %s)\" % (dtm_test.shape))\n    num_words = len([v for k,v in vectorizer.vocabulary_.items()]) + 1\n#     print('vocab size:%s' % (num_words))\n    maxlen = 2000\n    x_train = dtm2wid(dtm_train, maxlen)\n    x_test = dtm2wid(dtm_test, maxlen)\n    \n    return dtm_train, dtm_test, x_train, y_train, x_test, y_test, num_words, maxlen\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Function for computing Naive Bayes Log-Count Ratios**\n\nThese ratios capture the probability of a word appearing in a document in one class (i.e., positive) versus another (i.e., negative).","metadata":{}},{"cell_type":"code","source":"def pr(dtm, y, y_i):\n    p = dtm[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)\n\n# nbratios = np.log(pr(dtm_train, y_train, 1)/pr(dtm_train, \n#                                                #y_train, 0))\n# print(nbratios)\n# nbratios = np.squeeze(np.asarray(nbratios))\n# print(nbratios)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\nfrom keras import regularizers\nfrom keras.models import Model, Sequential\nfrom keras.layers.core import Activation\nfrom keras.layers import Input, Embedding, Flatten, dot, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building NBSVM/NBLR model**\n\nNBSVM is an approach to text classification proposed by Wang and Manning that takes a linear model such as SVM (or logistic regression) and infuses it with Bayesian probabilities by replacing word count features with Naive Bayes log-count ratios.","metadata":{}},{"cell_type":"code","source":"# def NBSVM(num_words, maxlen, nbratios=None):\n#     embedding_mat = np.zeros((num_words, 1))\n#     for i in range(1, num_words): # skip 0, the padding value\n#         if nbratios is not None:\n#             # if log-count ratios are supplied, then it's NBSVM\n#             embedding_mat[i] = nbratios[i-1]\n#         else:\n#             # if log-count ratios are not supplied, \n#             # this reduces to a logistic regression\n#             embedding_mat[i] = 1\n            \n#     input_layer = Input(shape=(maxlen,))\n#     nb_layer = Embedding(num_words, 1, input_length=maxlen, weights=[embedding_mat],trainable=False)(input_layer)\n#     x = Embedding(num_words, 1, input_length=maxlen, \n#                   embeddings_initializer='glorot_normal')(input_layer)\n#     x = dot([nb_layer,x], axes=1)\n#     x = Flatten()(x)\n#     x = Dropout(0.5)(x)\n#     x = Dense(64, activation='relu')(x)\n#     x = Dense(2, kernel_regularizer=regularizers.l1_l2(l1=0, l2=0.0001))(x) \n#     x = Activation('linear')(x)\n    \n#     model = Model(inputs=input_layer, outputs=x)\n#     model.compile(loss='squared_hinge',\n#                        optimizer='adadelta', metrics=['accuracy'])\n\n#     return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def NBLR(num_words, maxlen, nbratios=None):\n    embedding_mat = np.zeros((num_words, 1))\n    for i in range(1, num_words): # skip 0, the padding value\n        if nbratios is not None:\n            # if log-count ratios are supplied, then it's NBSVM\n            embedding_mat[i] = nbratios[i-1]\n        else:\n            # if log-count ratios are not supplied, \n            # this reduces to a logistic regression\n            embedding_mat[i] = 1\n            \n    input_layer = Input(shape=(maxlen,))\n    nb_layer = Embedding(num_words, 1, input_length=maxlen, weights=[embedding_mat],trainable=False)(input_layer)\n    x = Embedding(num_words, 1, input_length=maxlen, \n                  embeddings_initializer='glorot_normal')(input_layer)\n    x = dot([nb_layer,x], axes=1)\n    x = Flatten()(x)\n    x = Activation('sigmoid')(x)\n    \n    model = Model(inputs=input_layer, outputs=x)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=Adam(learning_rate=0.001),\n                  metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Start learning with SVM**","metadata":{}},{"cell_type":"code","source":"# start = time.time()\n# dtm_train, dtm_test, x_train, y_train, x_test, y_test, num_words, maxlen = prepare_data(X,y)\n# model = NBSVM(num_words, maxlen)\n# history = model.fit(x_train, y_train,\n#           batch_size=32,\n#           epochs=6,\n#           validation_data=(x_test, y_test))\n\n# figure, axis = plt.subplots(2, 1, constrained_layout = True)\n# axis[0].plot(history.history['loss'])\n# axis[0].plot(history.history['val_loss'])\n# axis[0].set_title('model loss')\n# axis[0].set_xlabel('loss')\n# axis[0].set_ylabel('epoch')\n# axis[0].legend(['train', 'val'], loc='upper left')\n\n# axis[1].plot(history.history['accuracy'])\n# axis[1].plot(history.history['val_accuracy'])\n# axis[1].set_title('model accuracy')\n# axis[1].set_xlabel('accuracy')\n# axis[1].set_ylabel('epoch')\n# axis[1].legend(['train', 'val'], loc='upper left')\n\n# plt.show()\n# stop = time.time()\n# print(f'Begin: {start}, finish: {stop}, summary time: {stop - start}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Start learning with Logistic regression**","metadata":{}},{"cell_type":"code","source":"start = time.time()\ndtm_train, dtm_test, x_train, y_train, x_test, y_test, num_words, maxlen = prepare_data(X,y)\nmodel = NBLR(num_words, maxlen)\nhistory = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test, y_test))\n\nfigure, axis = plt.subplots(2, 1, constrained_layout = True)\naxis[0].plot(history.history['loss'])\naxis[0].plot(history.history['val_loss'])\naxis[0].set_title('model loss')\naxis[0].set_xlabel('epoch')\naxis[0].set_ylabel('loss')\naxis[0].legend(['train', 'val'], loc='upper left')\n\naxis[1].plot(history.history['accuracy'])\naxis[1].plot(history.history['val_accuracy'])\naxis[1].set_title('model accuracy')\naxis[1].set_xlabel('epoch')\naxis[1].set_ylabel('accuracy')\naxis[1].legend(['train', 'val'], loc='upper left')\n\nplt.show()\nstop = time.time()\nprint(f'Begin: {start}, finish: {stop}, summary time: {stop - start}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Start learning with NBSVM**","metadata":{}},{"cell_type":"code","source":"# start = time.time()\n# dtm_train, dtm_test, x_train, y_train, x_test, y_test, num_words, maxlen = prepare_data(X,y)\n# nbratios = np.log(pr(dtm_train, y_train, 1)/pr(dtm_train, y_train, 0))\n# nbratios = np.squeeze(np.asarray(nbratios))\n# model = NBSVM(num_words, maxlen, nbratios=nbratios)\n# history = model.fit(x_train, y_train,\n#           batch_size=32,\n#           epochs=10,\n#           validation_data=(x_test, y_test))\n# figure, axis = plt.subplots(2, 1, constrained_layout = True)\n\n# axis[0].plot(history.history['loss'])\n# axis[0].plot(history.history['val_loss'])\n# axis[0].set_title('model loss')\n# axis[0].set_xlabel('loss')\n# axis[0].set_ylabel('epoch')\n# axis[0].legend(['train', 'val'], loc='upper left')\n\n# axis[1].plot(history.history['accuracy'])\n# axis[1].plot(history.history['val_accuracy'])\n# axis[1].set_title('model accuracy')\n# axis[1].set_xlabel('accuracy')\n# axis[1].set_ylabel('epoch')\n# axis[1].legend(['train', 'val'], loc='upper left')\n\n# plt.show()\n# stop = time.time()\n# print(f'Begin: {start}, finish: {stop}, summary time: {stop - start}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Starting learning with NBLR**","metadata":{}},{"cell_type":"code","source":"start = time.time()\ndtm_train, dtm_test, x_train, y_train, x_test, y_test, num_words, maxlen = prepare_data(X,y)\nnbratios = np.log(pr(dtm_train, y_train, 1)/pr(dtm_train, y_train, 0))\nnbratios = np.squeeze(np.asarray(nbratios))\nmodel = NBLR(num_words, maxlen, nbratios=nbratios)\nhistory = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test, y_test))\nfigure, axis = plt.subplots(2, 1, constrained_layout = True)\n\naxis[0].plot(history.history['loss'])\naxis[0].plot(history.history['val_loss'])\naxis[0].set_title('model loss')\naxis[0].set_xlabel('epoch')\naxis[0].set_ylabel('loss')\naxis[0].legend(['train', 'val'], loc='upper left')\n\naxis[1].plot(history.history['accuracy'])\naxis[1].plot(history.history['val_accuracy'])\naxis[1].set_title('model accuracy')\naxis[1].set_xlabel('epoch')\naxis[1].set_ylabel('accuracy')\naxis[1].legend(['train', 'val'], loc='upper left')\n\nplt.show()\nstop = time.time()\nprint(f'Begin: {start}, finish: {stop}, summary time: {stop - start}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here comes Federated Learning with Flower simulation**","metadata":{}},{"cell_type":"code","source":"!pip install -U flwr[\"simulation\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import flwr as fl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FlowerClient(fl.client.NumPyClient):\n    def __init__(self, model, x_train, y_train, x_val, y_val) -> None:\n        self.model = model\n        self.x_train, self.y_train = x_train, y_train\n        self.x_val, self.y_val = x_val, y_val\n\n    def get_parameters(self):\n        return self.model.get_weights()\n\n    def fit(self, parameters, config):\n        self.model.set_weights(parameters)\n        history = self.model.fit(self.x_train, self.y_train, epochs=1, verbose=1, batch_size=32)\n        print(f\"History during fit round: {history.history}\")\n        print(f\"Len of self x_train {len(self.x_train)}\")\n        return self.model.get_weights(), len(self.x_train), {}\n\n    def evaluate(self, parameters, config):\n        self.model.set_weights(parameters)\n        loss, acc = self.model.evaluate(self.x_val, self.y_val, verbose=2)\n        #print(f\"loss {loss} and acc is {acc}\")\n        return loss, len(self.x_val), {\"accuracy\": acc}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def client_fn(cid: str) -> fl.client.Client:\n    dtm_train, dtm_test, X_train, y_train, X_test, y_test, num_words, maxlen = prepare_data(X,y)\n    partition_size = math.floor(len(X_train) / NUM_CLIENTS)\n    idx_from, idx_to = int(cid) * partition_size, (int(cid) + 1) * partition_size\n    X_train_cid = X_train[idx_from:idx_to]\n    dtm_train_cid = dtm_train[idx_from:idx_to]\n    y_train_cid = y_train[idx_from:idx_to]\n    #print(f'Shape of cid train X: {X_train_cid.shape}')\n    #print(f'Shape of cid train y: {y_train_cid.shape}')\n    #print(f'Shape of cid dtm train: {dtm_train_cid.shape}')\n    \n    partition_size = math.floor(len(X_test) / NUM_CLIENTS)\n    idx_from, idx_to = int(cid) * partition_size, (int(cid) + 1) * partition_size\n    X_test_cid = X_test[idx_from:idx_to]\n    dtm_test_cid = dtm_test[idx_from:idx_to]\n    y_test_cid = y_test[idx_from:idx_to]\n\n      \n    nbratios = np.log(pr(dtm_train_cid, y_train_cid, 1)/pr(dtm_train_cid, \n                                               y_train_cid, 0))\n    nbratios = np.squeeze(np.asarray(nbratios))\n    \n    model = NBLR(num_words,maxlen,nbratios=nbratios)\n#     model = NBLR(num_words,maxlen)\n    return FlowerClient(model, X_train_cid, y_train_cid, X_test_cid, y_test_cid)\n    \n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Tuple, Optional, Dict\nclass SaveModelStrategy(fl.server.strategy.FedAvg):\n#     def aggregate_fit(\n#         self,\n#         rnd: int,\n#         results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n#         failures: List[BaseException],\n#     ) -> Optional[fl.common.Weights]:\n#         aggregated_weights = super().aggregate_fit(rnd, results, failures)\n#         if aggregated_weights is not None and rnd == 10:\n#             # Save aggregated_weights\n#             print(f\"Saving round {rnd} aggregated_weights...\")\n#             np.savez(f\"round-{rnd}-nblr_for_sure-weights.npz\", *aggregated_weights)\n#         return aggregated_weights\n    \n#     def aggregate_fit(\n#         self,\n#         rnd: int,\n#         results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n#         failures: List[BaseException],\n#     ) -> Tuple[Optional[fl.common.Parameters], Dict[str,fl.common.Scalar]]:\n#         params, metrics = super().aggregate_fit(rnd, results, failures)\n   \n     def aggregate_evaluate(\n        self,\n        rnd: int,\n        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.EvaluateRes]],\n        failures: List[BaseException],\n    ) -> Optional[float]:\n        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n        if not results:\n            return None\n\n        # Weigh accuracy of each client by number of examples used\n        accuracies = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n        examples = [r.num_examples for _, r in results]\n\n        # Aggregate and print custom metric\n        accuracy_aggregated = sum(accuracies) / sum(examples)\n        print(f\"Round {rnd} accuracy aggregated from client results: {accuracy_aggregated}\")\n\n        # Call aggregate_evaluate from base class (FedAvg)\n        return super().aggregate_evaluate(rnd, results, failures)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here comes federated learning with logistic regression**","metadata":{}},{"cell_type":"code","source":"NUM_CLIENTS = 10\n\n# Start simulation\nfl.simulation.start_simulation(\n    client_fn=client_fn,\n    num_clients=NUM_CLIENTS,\n    num_rounds=10,\n    strategy=SaveModelStrategy()\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here comes FL for NBLR**","metadata":{}},{"cell_type":"code","source":"NUM_CLIENTS = 10\n\n\n# Start simulation\nfl.simulation.start_simulation(\n    client_fn=client_fn,\n    num_clients=NUM_CLIENTS,\n    num_rounds=10,\n    #strategy=strategy,\n    strategy=SaveModelStrategy(min_fit_clients=3,min_eval_clients=3,)\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}