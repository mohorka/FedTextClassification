{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport time\nimport os\nimport math\nimport random\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-28T08:59:45.986983Z","iopub.execute_input":"2022-05-28T08:59:45.987381Z","iopub.status.idle":"2022-05-28T08:59:46.004887Z","shell.execute_reply.started":"2022-05-28T08:59:45.987282Z","shell.execute_reply":"2022-05-28T08:59:46.003762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Activation, Flatten, Input, concatenate, Conv1D, GlobalMaxPooling1D, MaxPooling1D\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:46.007194Z","iopub.execute_input":"2022-05-28T08:59:46.008082Z","iopub.status.idle":"2022-05-28T08:59:48.537155Z","shell.execute_reply.started":"2022-05-28T08:59:46.008025Z","shell.execute_reply":"2022-05-28T08:59:48.53621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Text preparation and load data**","metadata":{}},{"cell_type":"code","source":"stop = set(stopwords.words('english'))\ndef clean_data(text):\n    #text = BeautifulSoup(text, \"html.parser\").get_text()\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub(r'\\W', ' ', text)\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text) \n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    \n    final = []\n    for word in text.split():\n        if word.strip().lower() not in stop and word.strip().lower().isalpha():\n            final.append(word.strip().lower())\n    text = \" \".join(final)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.556994Z","iopub.execute_input":"2022-05-28T08:59:49.557359Z","iopub.status.idle":"2022-05-28T08:59:49.567397Z","shell.execute_reply.started":"2022-05-28T08:59:49.557312Z","shell.execute_reply":"2022-05-28T08:59:49.566187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data():\n    df_train = pd.read_csv('../input/emotions-dataset-for-nlp/train.txt', names=['Text', 'Emotion'], sep=';')\n    df_val = pd.read_csv('../input/emotions-dataset-for-nlp/val.txt', names=['Text', 'Emotion'], sep=';')\n    df_test = pd.read_csv('../input/emotions-dataset-for-nlp/test.txt', names=['Text', 'Emotion'], sep=';')\n    df_test = df_test[df_test['Emotion'].isin(['sadness','anger','joy','fear'])]\n    df_val = df_val[df_val['Emotion'].isin(['sadness','anger','joy','fear'])]\n    df_train = df_train[df_train['Emotion'].isin(['sadness','anger','joy','fear'])]\n\n    X_train = df_train['Text'].apply(clean_data)\n    y_train = df_train['Emotion']\n\n    X_test = df_test['Text'].apply(clean_data)\n    y_test = df_test['Emotion']\n\n    X_val = df_val['Text'].apply(clean_data)\n    y_val = df_val['Emotion']\n    \n    le = LabelEncoder()\n    y_train = le.fit_transform(y_train)\n    y_test = le.transform(y_test)\n    y_val = le.transform(y_val)\n\n    y_train = to_categorical(y_train)\n    y_test = to_categorical(y_test)\n    y_val = to_categorical(y_val)\n    \n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))\n\n    sequences_train = tokenizer.texts_to_sequences(X_train)\n    sequences_test = tokenizer.texts_to_sequences(X_test)\n    sequences_val = tokenizer.texts_to_sequences(X_val)\n\n    X_train = pad_sequences(sequences_train, maxlen=256, truncating='pre')\n    X_test = pad_sequences(sequences_test, maxlen=256, truncating='pre')\n    X_val = pad_sequences(sequences_val, maxlen=256, truncating='pre')\n\n    vocabSize = len(tokenizer.index_word) + 1\n    print(f\"Vocabulary size = {vocabSize}\")\n    \n    return X_train, y_train, X_val, y_val, X_test, y_test, vocabSize","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.572497Z","iopub.execute_input":"2022-05-28T08:59:49.572724Z","iopub.status.idle":"2022-05-28T08:59:49.585066Z","shell.execute_reply.started":"2022-05-28T08:59:49.572696Z","shell.execute_reply":"2022-05-28T08:59:49.584083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Count F-score**","metadata":{}},{"cell_type":"code","source":"def get_f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.586416Z","iopub.execute_input":"2022-05-28T08:59:49.586675Z","iopub.status.idle":"2022-05-28T08:59:49.604129Z","shell.execute_reply.started":"2022-05-28T08:59:49.586645Z","shell.execute_reply":"2022-05-28T08:59:49.603209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Start learning for CNN+bi-LSTM model**","metadata":{}},{"cell_type":"code","source":"start = time.time()\n# Data\nX_train, y_train, X_val, y_val, X_test, y_test, vocabSize = load_data()\n# Embedding\nmax_features = vocabSize\nmaxlen = X_train.shape[1]\nembedding_size = 256\n\n# Convolution\nkernel_size = 5\nfilters = 128\npool_size = 4\n\n# LSTM\nlstm_output_size = 128\n\nprint('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(vocabSize, embedding_size, input_length=X_train.shape[1]))\nmodel.add(Dropout(0.25))\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\nmodel.add(MaxPooling1D(pool_size=pool_size))\n#model.add(LSTM(lstm_output_size))\nmodel.add(Bidirectional(LSTM(lstm_output_size)))\nmodel.add(Dense(4))\nmodel.add(Activation('softmax'))\nmodel.summary()\n\nadam = Adam(learning_rate=10e-3)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=get_f1)\n\n# Fit model\nhistory = model.fit(X_train,\n                    y_train,\n                    validation_data=(X_val, y_val),\n                    verbose=1,\n                    batch_size=256,\n                    epochs=3\n                   )\n\nmodel.evaluate(X_test, y_test, verbose=1)\n_time = time.time() - start\nprint(f\"Learning time: {_time}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.605779Z","iopub.execute_input":"2022-05-28T08:59:49.606039Z","iopub.status.idle":"2022-05-28T08:59:49.625523Z","shell.execute_reply.started":"2022-05-28T08:59:49.606008Z","shell.execute_reply":"2022-05-28T08:59:49.62452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss Function')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.640851Z","iopub.execute_input":"2022-05-28T08:59:49.641225Z","iopub.status.idle":"2022-05-28T08:59:49.653621Z","shell.execute_reply.started":"2022-05-28T08:59:49.641188Z","shell.execute_reply":"2022-05-28T08:59:49.652598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['get_f1'])\nplt.plot(history.history['val_get_f1'])\nplt.title('model F1-score')\nplt.ylabel('F1-score')\nplt.xlabel('epoch')\nplt.legend(['train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.654666Z","iopub.execute_input":"2022-05-28T08:59:49.654907Z","iopub.status.idle":"2022-05-28T08:59:49.665636Z","shell.execute_reply.started":"2022-05-28T08:59:49.654877Z","shell.execute_reply":"2022-05-28T08:59:49.664658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Classify custom sample\n# def predict(sentence):\n#     print(sentence)\n#     sentence = clean(sentence)\n#     sentence = tokenizer.texts_to_sequences([sentence])\n#     sentence = pad_sequences(sentence, maxlen=256, truncating='pre')\n#     result = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\n#     proba =  np.max(model.predict(sentence))\n    \n#     print(f\"{result} : {proba}\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.667421Z","iopub.execute_input":"2022-05-28T08:59:49.667762Z","iopub.status.idle":"2022-05-28T08:59:49.682033Z","shell.execute_reply.started":"2022-05-28T08:59:49.667717Z","shell.execute_reply":"2022-05-28T08:59:49.68113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.696597Z","iopub.execute_input":"2022-05-28T08:59:49.697406Z","iopub.status.idle":"2022-05-28T08:59:49.708147Z","shell.execute_reply.started":"2022-05-28T08:59:49.697354Z","shell.execute_reply":"2022-05-28T08:59:49.707251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define Transformer block**","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.709889Z","iopub.execute_input":"2022-05-28T08:59:49.7102Z","iopub.status.idle":"2022-05-28T08:59:49.724378Z","shell.execute_reply.started":"2022-05-28T08:59:49.71016Z","shell.execute_reply":"2022-05-28T08:59:49.723352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Start learning for Transformer+CNN**","metadata":{}},{"cell_type":"code","source":"start = time.time()\nembed_dim = 64  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nff_dim = 64 # Hidden layer size in feed forward network inside transformer\nX_train, y_train, X_val, y_val, X_test, y_test, vocab_size = load_data()\nmaxlen = X_train.shape[1]\nkernel_size = 8\nfilters = 32\n\ninputs = layers.Input(shape=(maxlen,))\nembedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.Conv1D(16,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1)(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\n# x = layers.Dense(16, activation=\"relu\")(x)\n# x = layers.Dropout(0.1)(x)\noutputs = layers.Dense(4, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[get_f1])\nmodel.summary()\nhistory = model.fit(\n    X_train, y_train, batch_size=32, epochs=3, validation_data=(X_val, y_val)\n)\nfinish = time.time()\nprint(f'Finished at {finish-start}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['get_f1'])\nplt.plot(history.history['val_get_f1'])\nplt.title('model F1-score')\nplt.ylabel('F1-score')\nplt.xlabel('epoch')\nplt.legend(['train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.76094Z","iopub.execute_input":"2022-05-28T08:59:49.76199Z","iopub.status.idle":"2022-05-28T08:59:49.777122Z","shell.execute_reply.started":"2022-05-28T08:59:49.761919Z","shell.execute_reply":"2022-05-28T08:59:49.7764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss Function')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T08:59:49.778444Z","iopub.execute_input":"2022-05-28T08:59:49.779353Z","iopub.status.idle":"2022-05-28T08:59:49.790502Z","shell.execute_reply.started":"2022-05-28T08:59:49.779302Z","shell.execute_reply":"2022-05-28T08:59:49.789726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define federated client**\n","metadata":{}},{"cell_type":"code","source":"!pip install -U flwr[\"simulation\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make TensorFlow logs less verbose\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\nimport flwr as fl","metadata":{"execution":{"iopub.status.busy":"2022-05-28T09:00:00.556724Z","iopub.execute_input":"2022-05-28T09:00:00.557004Z","iopub.status.idle":"2022-05-28T09:00:00.794694Z","shell.execute_reply.started":"2022-05-28T09:00:00.556964Z","shell.execute_reply":"2022-05-28T09:00:00.793524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FlowerClient(fl.client.NumPyClient):\n    def __init__(self, model, x_train, y_train, x_val, y_val) -> None:\n        self.model = model\n        self.x_train, self.y_train = x_train, y_train\n        self.x_val, self.y_val = x_val, y_val\n\n    def get_parameters(self):\n        return self.model.get_weights()\n\n    def fit(self, parameters, config):\n        self.model.set_weights(parameters)\n        self.model.fit(self.x_train, self.y_train, epochs=1, verbose=1, batch_size=16)\n        print(f\"Len of self x_train {len(self.x_train)}\")\n        return self.model.get_weights(), len(self.x_train), {}\n\n    def evaluate(self, parameters, config):\n        self.model.set_weights(parameters)\n        loss, acc = self.model.evaluate(self.x_val, self.y_val, verbose=2)\n        print(f\"loss {loss} and acc is {acc}\")\n        return loss, len(self.x_val), {\"accuracy\": acc}","metadata":{"execution":{"iopub.status.busy":"2022-05-28T09:00:00.796018Z","iopub.execute_input":"2022-05-28T09:00:00.796271Z","iopub.status.idle":"2022-05-28T09:00:00.805618Z","shell.execute_reply.started":"2022-05-28T09:00:00.796242Z","shell.execute_reply":"2022-05-28T09:00:00.804426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Federated client with CNN+bi-LSTM model**","metadata":{}},{"cell_type":"code","source":"def client_fn(cid: str) -> fl.client.Client:\n    X_train, y_train, X_val, y_val, X_test, y_test, vocabSize = load_data()\n    \n    max_features = vocabSize\n    maxlen = X_train.shape[1]\n    embedding_size = 256\n    print(f'max length of embedding {maxlen}')\n    print(f'max features/vocabSize {vocabSize}')\n    print(f'Len of X train is {len(X_train)}')\n    print(f'Len of y train is {len(y_train)}')\n    print(f'Len of X val is {len(X_val)}')\n    print(f'Len of y val is {len(y_val)}')\n    \n    # Convolution\n    kernel_size = 5\n    filters = 128\n    pool_size = 4\n\n    # LSTM\n    lstm_output_size = 128\n    \n    #Get train data for particular client\n    partition_size = math.floor(len(X_train) / NUM_CLIENTS)\n    idx_from, idx_to = int(cid) * partition_size, (int(cid) + 1) * partition_size\n    X_train_cid = X_train[idx_from:idx_to]\n    y_train_cid = y_train[idx_from:idx_to]\n    print(f'Len of cid train X: {len(X_train_cid)}')\n    print(f'Len of cid train y: {len(y_train_cid)}')\n    \n    #Get validation data for particular client\n    partition_size = math.floor(len(X_val) / NUM_CLIENTS)\n    idx_from, idx_to = int(cid) * partition_size, (int(cid) + 1) * partition_size\n    X_val_cid = X_val[idx_from:idx_to]\n    y_val_cid = y_val[idx_from:idx_to]\n    print(f'Len of cid train X: {len(X_val_cid)}')\n    print(f'Len of cid train y: {len(y_val_cid)}')\n    \n      # Create model\n    model = Sequential(\n        [\n            Embedding(vocabSize, embedding_size, input_length=X_train.shape[1]),\n            Dropout(0.25),\n            Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1),\n            MaxPooling1D(pool_size=pool_size),\n            Bidirectional(LSTM(lstm_output_size)),\n            Dense(4),\n            Activation('softmax')\n        ]\n    )\n    model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    return FlowerClient(model, X_train_cid, y_train_cid, X_val_cid, y_val_cid)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T09:00:00.807556Z","iopub.execute_input":"2022-05-28T09:00:00.807927Z","iopub.status.idle":"2022-05-28T09:00:00.826363Z","shell.execute_reply.started":"2022-05-28T09:00:00.807883Z","shell.execute_reply":"2022-05-28T09:00:00.82527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Tuple, Optional","metadata":{"execution":{"iopub.status.busy":"2022-05-28T09:00:00.827912Z","iopub.execute_input":"2022-05-28T09:00:00.828432Z","iopub.status.idle":"2022-05-28T09:00:00.844727Z","shell.execute_reply.started":"2022-05-28T09:00:00.828389Z","shell.execute_reply":"2022-05-28T09:00:00.843629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define federated strategy to have ability to save weights or to input aggregated metrics**","metadata":{}},{"cell_type":"code","source":"class SaveModelStrategy(fl.server.strategy.FedAvg):\n#     def aggregate_fit(\n#         self,\n#         rnd: int,\n#         results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n#         failures: List[BaseException],\n#     ) -> Optional[fl.common.Weights]:\n#         aggregated_weights = super().aggregate_fit(rnd, results, failures)\n#         if aggregated_weights is not None:\n#             # Save aggregated_weights\n#             print(f\"Saving round {rnd} aggregated_weights...\")\n#             np.savez(f\"round-{rnd}-weights.npz\", *aggregated_weights)\n#         return aggregated_weights\n    def aggregate_evaluate(\n        self,\n        rnd: int,\n        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.EvaluateRes]],\n        failures: List[BaseException],\n    ) -> Optional[float]:\n        if not results:\n            return None\n\n        # Weigh accuracy of each client by number of examples used\n        accuracies = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n        examples = [r.num_examples for _, r in results]\n\n        # Aggregate and print custom metric\n        accuracy_aggregated = sum(accuracies) / sum(examples)\n        print(f\"Round {rnd} accuracy aggregated from client results: {accuracy_aggregated}\")\n\n        # Call aggregate_evaluate from base class (FedAvg)\n        return super().aggregate_evaluate(rnd, results, failures)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T09:00:00.845922Z","iopub.execute_input":"2022-05-28T09:00:00.846302Z","iopub.status.idle":"2022-05-28T09:00:00.866306Z","shell.execute_reply.started":"2022-05-28T09:00:00.846255Z","shell.execute_reply":"2022-05-28T09:00:00.865262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Start federated learning with CNN+bi-LSTM model**","metadata":{}},{"cell_type":"code","source":"NUM_CLIENTS = 15\n\n# Start simulation\nfl.simulation.start_simulation(\n    client_fn=client_fn,\n    num_clients=NUM_CLIENTS,\n    num_rounds=10,\n    strategy = SaveModelStrategy(min_fit_clients=2,min_eval_clients=2)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Federated client with Transformer+CNN**","metadata":{}},{"cell_type":"code","source":"def client_fn_transformer(cid: str) -> fl.client.Client:\n    embed_dim = 64  # Embedding size for each token\n    num_heads = 2  # Number of attention heads\n    ff_dim = 64 # Hidden layer size in feed forward network inside transformer\n    \n    X_train, y_train, X_val, y_val, X_test, y_test, vocab_size = load_data()\n    \n    maxlen = X_train.shape[1]\n    kernel_size = 4\n    filters = 32\n\n    inputs = layers.Input(shape=(maxlen,))\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n    x = transformer_block(x)\n    x = layers.Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(16, activation=\"relu\")(x)\n    #x = layers.Dropout(0.1)(x)\n    outputs = layers.Dense(4, activation=\"softmax\")(x)\n\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[get_f1])\n    \n    #Get train data for particular client\n    partition_size = math.floor(len(X_train) / NUM_CLIENTS)\n    idx_from, idx_to = int(cid) * partition_size, (int(cid) + 1) * partition_size\n    X_train_cid = X_train[idx_from:idx_to]\n    y_train_cid = y_train[idx_from:idx_to]\n    print(f'Len of cid train X: {len(X_train_cid)}')\n    print(f'Len of cid train y: {len(y_train_cid)}')\n    \n    #Get validation data for particular client\n    partition_size = math.floor(len(X_val) / NUM_CLIENTS)\n    idx_from, idx_to = int(cid) * partition_size, (int(cid) + 1) * partition_size\n    X_val_cid = X_val[idx_from:idx_to]\n    y_val_cid = y_val[idx_from:idx_to]\n    print(f'Len of cid train X: {len(X_val_cid)}')\n    print(f'Len of cid train y: {len(y_val_cid)}')\n    \n   \n\n    return FlowerClient(model, X_train_cid, y_train_cid, X_val_cid, y_val_cid)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T09:05:04.140833Z","iopub.execute_input":"2022-05-28T09:05:04.141414Z","iopub.status.idle":"2022-05-28T09:05:04.154517Z","shell.execute_reply.started":"2022-05-28T09:05:04.14136Z","shell.execute_reply":"2022-05-28T09:05:04.153563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Start federated learning with Transformer+CNN**","metadata":{}},{"cell_type":"code","source":"NUM_CLIENTS = 15\n\n# Start simulation\nfl.simulation.start_simulation(\n    client_fn=client_fn_transformer,\n    num_clients=NUM_CLIENTS,\n    num_rounds=10,\n    strategy = SaveModelStrategy(min_fit_clients=2,min_eval_clients=2)\n)","metadata":{},"execution_count":null,"outputs":[]}]}